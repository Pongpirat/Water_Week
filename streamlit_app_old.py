import streamlit as st
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
import altair as alt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def load_data(file):
    return pd.read_csv(file)

def clean_data(df):
    data_clean = df.copy()
    data_clean['datetime'] = pd.to_datetime(data_clean['datetime'], errors='coerce')
    data_clean = data_clean.dropna(subset=['datetime'])
    data_clean = data_clean[(data_clean['wl_up'] >= 100) & (data_clean['wl_up'] <= 450)]
    data_clean = data_clean[(data_clean['wl_up'] != 0) & (~data_clean['wl_up'].isna())]
    return data_clean

def create_time_features(data_clean):
    if not pd.api.types.is_datetime64_any_dtype(data_clean['datetime']):
        data_clean['datetime'] = pd.to_datetime(data_clean['datetime'], errors='coerce')

    data_clean['year'] = data_clean['datetime'].dt.year
    data_clean['month'] = data_clean['datetime'].dt.month
    data_clean['day'] = data_clean['datetime'].dt.day
    data_clean['hour'] = data_clean['datetime'].dt.hour
    data_clean['minute'] = data_clean['datetime'].dt.minute
    data_clean['day_of_week'] = data_clean['datetime'].dt.dayofweek
    data_clean['day_of_year'] = data_clean['datetime'].dt.dayofyear
    data_clean['week_of_year'] = data_clean['datetime'].dt.isocalendar().week
    data_clean['days_in_month'] = data_clean['datetime'].dt.days_in_month

    return data_clean

def prepare_features(data_clean):
    feature_cols = [
        'year', 'month', 'day', 'hour', 'minute',
        'day_of_week', 'day_of_year', 'week_of_year',
        'days_in_month'
    ]
    X = data_clean[feature_cols]
    y = data_clean['wl_up']
    return X, y

def train_model(X_train, y_train):
    param_distributions = {
        'n_estimators': [100, 200, 500, 1000],  # à¸‚à¸¢à¸²à¸¢à¸Šà¹ˆà¸§à¸‡à¸‚à¸­à¸‡ n_estimators
        'max_depth': [None, 10, 20, 50],  # à¸›à¸£à¸±à¸š max_depth à¹ƒà¸«à¹‰à¸à¸§à¹‰à¸²à¸‡à¸‚à¸¶à¹‰à¸™
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['auto', 'sqrt', 'log2'],
        'bootstrap': [True, False],  # à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸•à¹‰à¸™à¹„à¸¡à¹‰à¸«à¸£à¸·à¸­à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™
        'max_samples': [None, 0.8, 0.9]  # à¸à¸³à¸«à¸™à¸” max_samples à¹€à¸à¸·à¹ˆà¸­à¸ªà¸¸à¹ˆà¸¡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸•à¹‰à¸™à¹„à¸¡à¹‰
    }

    rf = RandomForestRegressor(random_state=42)

    n_splits = min(3, len(X_train) // 2)
    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, n_iter=20, cv=n_splits, n_jobs=-1, verbose=2, random_state=42)
    random_search.fit(X_train, y_train)

    return random_search.best_estimator_

def generate_missing_dates(data):
    full_date_range = pd.date_range(start=data['datetime'].min(), end=data['datetime'].max(), freq='15T')
    all_dates = pd.DataFrame(full_date_range, columns=['datetime'])
    data_with_all_dates = pd.merge(all_dates, data, on='datetime', how='left')
    return data_with_all_dates

def fill_code_column(data):
    data['code'] = data['code'].fillna(method='ffill').fillna(method='bfill')
    return data

def smooth_filled_values(data_with_all_dates, window_size=3):
    """Apply smoothing technique to reduce the sudden jumps in the filled values."""
    data_with_all_dates['wl_up'] = data_with_all_dates['wl_up'].interpolate(method='linear')
    data_with_all_dates['wl_up'] = data_with_all_dates['wl_up'].rolling(window=window_size, min_periods=1).mean()
    return data_with_all_dates

def handle_missing_values_by_week(data_clean, start_date, end_date):
    feature_cols = ['year', 'month', 'day', 'hour', 'minute',
                    'day_of_week', 'day_of_year', 'week_of_year', 'days_in_month']

    data = data_clean.copy()
    
    # Convert start_date and end_date to datetime
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)

    # Filter data based on the datetime range
    data = data[(data['datetime'] >= start_date) & (data['datetime'] <= end_date)]

    # Generate all missing dates within the selected range
    data_with_all_dates = generate_missing_dates(data)
    data_with_all_dates.index = pd.to_datetime(data_with_all_dates['datetime'])
    data_missing = data_with_all_dates[data_with_all_dates['wl_up'].isnull()]
    data_not_missing = data_with_all_dates.dropna(subset=['wl_up'])

    # à¹€à¸à¸´à¹ˆà¸¡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ timestamp à¹à¸¥à¸° wl_forecast
    data_with_all_dates['timestamp'] = pd.NaT  # à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹€à¸›à¹‡à¸™ NaT (Not a Timestamp)
    data_with_all_dates['wl_forecast'] = np.nan  # à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸–à¸¹à¸à¹€à¸•à¸´à¸¡

    if len(data_missing) == 0:
        st.write("No missing values to predict.")
        return data_with_all_dates

    # Train initial model with all available data
    X_train, y_train = prepare_features(data_not_missing)
    model = train_model(X_train, y_train)

    # Separate weeks with fewer and more missing rows
    weeks_with_fewer_missing = []
    weeks_with_more_missing = []

    weeks_with_missing = data_missing['week_of_year'].unique()

    for week in weeks_with_missing:
        group = data_missing[data_missing['week_of_year'] == week]
        missing_count = group['wl_up'].isnull().sum()
        if missing_count <= 288:
            weeks_with_fewer_missing.append(week)
        else:
            weeks_with_more_missing.append(week)

    # Handle weeks with fewer than 288 missing rows by predicting missing values row-by-row
    for week in weeks_with_fewer_missing:
        group = data_missing[data_missing['week_of_year'] == week]
        week_data = data_not_missing[data_not_missing['week_of_year'] == week]

        if len(week_data) > 0:
            X_train_week, y_train_week = prepare_features(week_data)
            model_week = train_model(X_train_week, y_train_week)

            for idx, row in group.iterrows():
                X_missing = row[feature_cols].values.reshape(1, -1)
                predicted_value = model_week.predict(X_missing)
                
                # à¸šà¸±à¸™à¸—à¸¶à¸à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¹€à¸•à¸´à¸¡à¹ƒà¸™à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ wl_forecast à¹à¸¥à¸° timestamp
                data_with_all_dates.loc[idx, 'wl_forecast'] = predicted_value
                data_with_all_dates.loc[idx, 'timestamp'] = pd.Timestamp.now()

    # Update data_not_missing after filling values
    data_not_missing = data_with_all_dates.dropna(subset=['wl_up'])

    # Handle weeks with more than 288 missing rows using data from adjacent weeks
    for week in weeks_with_more_missing:
        group = data_missing[data_missing['week_of_year'] == week]
        prev_week = week - 1 if week > min(weeks_with_missing) else week
        next_week = week + 1 if week < max(weeks_with_missing) else week

        prev_data = data_not_missing[data_not_missing['week_of_year'] == prev_week]
        next_data = data_not_missing[data_not_missing['week_of_year'] == next_week]
        previous_month_data = data_not_missing[data_not_missing['month'] == group['month'].iloc[0] - 1]

        combined_data = pd.concat([prev_data, next_data, previous_month_data])

        if data_missing[data_missing['week_of_year'] == next_week]['wl_up'].isnull().sum() > 288:
            current_month = group['month'].iloc[0]
            non_missing_month_data = data_clean[(data_clean['month'] == current_month) & (~data_clean['wl_up'].isnull())]

            X_train_month, y_train_month = prepare_features(non_missing_month_data)
            model_month = train_model(X_train_month, y_train_month)

            for idx, row in group.iterrows():
                X_missing = row[feature_cols].values.reshape(1, -1)
                predicted_value = model_month.predict(X_missing)

                # à¸šà¸±à¸™à¸—à¸¶à¸à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¹€à¸•à¸´à¸¡à¹ƒà¸™à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ wl_forecast à¹à¸¥à¸° timestamp
                data_with_all_dates.loc[idx, 'wl_forecast'] = predicted_value
                data_with_all_dates.loc[idx, 'timestamp'] = pd.Timestamp.now()
        else:
            combined_train_X, combined_train_y = prepare_features(combined_data)
            model_combined = train_model(combined_train_X, combined_train_y)

            for idx, row in group.iterrows():
                X_missing = row[feature_cols].values.reshape(1, -1)
                predicted_value = model_combined.predict(X_missing)

                # à¸šà¸±à¸™à¸—à¸¶à¸à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¹€à¸•à¸´à¸¡à¹ƒà¸™à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ wl_forecast à¹à¸¥à¸° timestamp
                data_with_all_dates.loc[idx, 'wl_forecast'] = predicted_value
                data_with_all_dates.loc[idx, 'timestamp'] = pd.Timestamp.now()

    # à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ wl_up2 à¸—à¸µà¹ˆà¸£à¸§à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡à¸à¸±à¸šà¸„à¹ˆà¸²à¸—à¸µà¹ˆà¹€à¸•à¸´à¸¡
    data_with_all_dates['wl_up2'] = data_with_all_dates['wl_up'].combine_first(data_with_all_dates['wl_forecast'])

    data_with_all_dates.reset_index(drop=True, inplace=True)
    return data_with_all_dates

def delete_data_by_date_range(data, delete_start_date, delete_end_date):
    # Convert delete_start_date and delete_end_date to datetime
    delete_start_date = pd.to_datetime(delete_start_date)
    delete_end_date = pd.to_datetime(delete_end_date)

    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸Šà¹ˆà¸§à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸Šà¹ˆà¸§à¸‡à¸‚à¸­à¸‡ data à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
    data_to_delete = data[(data['datetime'] >= delete_start_date) & (data['datetime'] <= delete_end_date)]

    # à¹€à¸à¸´à¹ˆà¸¡à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸–à¹‰à¸²à¸ˆà¸³à¸™à¸§à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸–à¸¹à¸à¸¥à¸šà¸¡à¸µà¸¡à¸²à¸à¹€à¸à¸´à¸™à¹„à¸›
    if len(data_to_delete) == 0:
        st.warning(f"à¹„à¸¡à¹ˆà¸à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ {delete_start_date} à¹à¸¥à¸° {delete_end_date}.")
    elif len(data_to_delete) > (0.3 * len(data)):  # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸–à¹‰à¸²à¸¥à¸šà¹€à¸à¸´à¸™ 30% à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
        st.warning("à¸„à¸³à¹€à¸•à¸·à¸­à¸™: à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¡à¸²à¸à¹€à¸à¸´à¸™à¹„à¸›à¸—à¸µà¹ˆà¸ˆà¸°à¸¥à¸š à¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£à¸¥à¸šà¸–à¸¹à¸à¸¢à¸à¹€à¸¥à¸´à¸")
    else:
        # à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸”à¸¢à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² wl_up à¹€à¸›à¹‡à¸™ NaN
        data.loc[data_to_delete.index, 'wl_up'] = np.nan

    return data

def calculate_accuracy_metrics(original, filled):
    # à¸œà¸ªà¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ datetime à¹€à¸›à¹‡à¸™à¸à¸·à¹‰à¸™à¸à¸²à¸™ à¹‚à¸”à¸¢à¸ˆà¸°à¹ƒà¸Šà¹‰ wl_up à¸ˆà¸²à¸ original à¹à¸¥à¸° wl_up2 à¸ˆà¸²à¸ filled
    merged_data = pd.merge(original[['datetime', 'wl_up']], filled[['datetime', 'wl_up2']], on='datetime')
    
    # à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸ˆà¸²à¸ wl_up à¸‚à¸­à¸‡ original à¹à¸¥à¸° wl_up2 à¸‚à¸­à¸‡ filled
    mse = mean_squared_error(merged_data['wl_up'], merged_data['wl_up2'])
    mae = mean_absolute_error(merged_data['wl_up'], merged_data['wl_up2'])
    r2 = r2_score(merged_data['wl_up'], merged_data['wl_up2'])

    # à¹à¸ªà¸”à¸‡à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸šà¸™à¸«à¸™à¹‰à¸²à¸ˆà¸­
    st.header("à¸œà¸¥à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³", divider='gray')

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric(label="Mean Squared Error (MSE)", value=f"{mse:.4f}")

    with col2:
        st.metric(label="Mean Absolute Error (MAE)", value=f"{mae:.4f}")

    with col3:
        st.metric(label="R-squared (RÂ²)", value=f"{r2:.4f}")

def plot_results(data_before, data_filled, data_deleted):
    data_before_filled = pd.DataFrame({
        'à¸§à¸±à¸™à¸—à¸µà¹ˆ': data_before['datetime'],
        'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡': data_before['wl_up']
    })

    data_after_filled = pd.DataFrame({
        'à¸§à¸±à¸™à¸—à¸µà¹ˆ': data_filled['datetime'],
        'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²': data_filled['wl_up2']
    })

    data_after_deleted = pd.DataFrame({
        'à¸§à¸±à¸™à¸—à¸µà¹ˆ': data_deleted['datetime'],
        'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¸¥à¸š': data_deleted['wl_up']
    })

    combined_data = pd.merge(data_before_filled, data_after_filled, on='à¸§à¸±à¸™à¸—à¸µà¹ˆ', how='outer')
    combined_data = pd.merge(combined_data, data_after_deleted, on='à¸§à¸±à¸™à¸—à¸µà¹ˆ', how='outer')

    min_y = combined_data[['à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡', 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²', 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¸¥à¸š']].min().min()
    max_y = combined_data[['à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡', 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²', 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¸¥à¸š']].max().max()

    chart = alt.Chart(combined_data).transform_fold(
        ['à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡', 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²', 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¸¥à¸š'],
        as_=['à¸‚à¹‰à¸­à¸¡à¸¹à¸¥', 'à¸£à¸°à¸”à¸±à¸šà¸™à¹‰à¸³']
    ).mark_line().encode(
        x='à¸§à¸±à¸™à¸—à¸µà¹ˆ:T',
        y=alt.Y('à¸£à¸°à¸”à¸±à¸šà¸™à¹‰à¸³:Q', scale=alt.Scale(domain=[min_y, max_y])),
        color=alt.Color('à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:N', scale=alt.Scale(scheme='reds'), legend=alt.Legend(orient='right', title='à¸‚à¹‰à¸­à¸¡à¸¹à¸¥'))
    ).properties(
        height=400
    ).interactive()

    st.header("à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸à¸²à¸£à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸«à¸²à¸¢à¹„à¸›", divider='gray')
    st.altair_chart(chart, use_container_width=True)

    st.header("à¸•à¸²à¸£à¸²à¸‡à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²", divider='gray')
    data_filled_selected = data_filled[['code', 'datetime', 'wl_up', 'wl_forecast', 'rf_15m', 'timestamp']]
    st.dataframe(data_filled_selected, use_container_width=True)

    # à¹€à¸£à¸µà¸¢à¸à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³
    calculate_accuracy_metrics(data_before, data_filled)

def plot_data_preview(data1, data2):
    data_pre1 = pd.DataFrame({
        'à¸§à¸±à¸™à¸—à¸µà¹ˆ': data1['datetime'],
        'à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²': data1['wl_up']
    })

    data_pre2 = pd.DataFrame({
        'à¸§à¸±à¸™à¸—à¸µà¹ˆ': data2['datetime'],
        'à¸ªà¸–à¸²à¸™à¸µà¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²': data2['wl_up']
    })

    combined_data_pre = pd.merge(data_pre1, data_pre2, on='à¸§à¸±à¸™à¸—à¸µà¹ˆ', how='outer')
    min_y = combined_data_pre[['à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²', 'à¸ªà¸–à¸²à¸™à¸µà¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²']].min().min()
    max_y = combined_data_pre[['à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²', 'à¸ªà¸–à¸²à¸™à¸µà¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²']].max().max()

    chart = alt.Chart(combined_data_pre).transform_fold(
        ['à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²', 'à¸ªà¸–à¸²à¸™à¸µà¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²'],
        as_=['à¸‚à¹‰à¸­à¸¡à¸¹à¸¥', 'à¸£à¸°à¸”à¸±à¸šà¸™à¹‰à¸³']
    ).mark_line().encode(
        x='à¸§à¸±à¸™à¸—à¸µà¹ˆ:T',
        y=alt.Y('à¸£à¸°à¸”à¸±à¸šà¸™à¹‰à¸³:Q', scale=alt.Scale(domain=[min_y, max_y])),
        color=alt.Color('à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:N', scale=alt.Scale(scheme='reds'), legend=alt.Legend(orient='right', title='à¸‚à¹‰à¸­à¸¡à¸¹à¸¥'))  # à¹ƒà¸Šà¹‰à¸à¸²à¹€à¸¥à¸• magma à¹à¸¥à¸°à¸›à¸£à¸±à¸šà¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ Legend à¹„à¸›à¸—à¸²à¸‡à¸‚à¸§à¸²
    ).properties(
        height=400,
        title='à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸–à¸²à¸™à¸µ'
    ).interactive()

    st.altair_chart(chart, use_container_width=True)

# Streamlit UI
st.set_page_config(
    page_title="RandomForest",
    page_icon="ğŸŒ²",
    layout="wide"
)
'''
# à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸°à¸”à¸±à¸šà¸™à¹‰à¸³à¸”à¹‰à¸§à¸¢ Random Forest
à¹à¸­à¸› Streamlit à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸°à¸”à¸±à¸šà¸™à¹‰à¸³ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥ Random Forest à¹€à¸à¸·à¹ˆà¸­à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸‚à¸²à¸”à¸«à¸²à¸¢à¹„à¸› 
à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸–à¸¹à¸à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸¥à¸°à¹à¸ªà¸”à¸‡à¸œà¸¥à¸œà¹ˆà¸²à¸™à¸à¸£à¸²à¸Ÿà¹à¸¥à¸°à¸à¸²à¸£à¸§à¸±à¸”à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸¥à¸·à¸­à¸à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ, 
à¸à¸³à¸«à¸™à¸”à¸Šà¹ˆà¸§à¸‡à¹€à¸§à¸¥à¸²à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥, à¹à¸¥à¸°à¸”à¸¹à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸‚à¸­à¸‡à¸à¸²à¸£à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²à¹„à¸”à¹‰
'''
st.markdown("---")

# Sidebar: Upload files and choose date ranges
with st.sidebar:
    st.header("à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ CSV")
    
    with st.sidebar.expander("à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸–à¸²à¸™à¸µà¸§à¸±à¸”à¸£à¸°à¸”à¸±à¸šà¸™à¹‰à¸³", expanded=False):
        uploaded_file = st.file_uploader("à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²", type="csv", key="uploader1")
        uploaded_file2 = st.file_uploader("à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥", type="csv", key="uploader2")

    # à¹€à¸¥à¸·à¸­à¸à¸Šà¹ˆà¸§à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆà¹ƒà¸™ sidebar
    st.header("à¹€à¸¥à¸·à¸­à¸à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥")
    start_date = st.date_input("à¸§à¸±à¸™à¸—à¸µà¹ˆà¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™", value=pd.to_datetime("2024-05-01"))
    end_date = st.date_input("à¸§à¸±à¸™à¸—à¸µà¹ˆà¸ªà¸´à¹‰à¸™à¸ªà¸¸à¸”", value=pd.to_datetime("2024-05-31"))
    
    # à¹€à¸à¸´à¹ˆà¸¡à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸§à¹ˆà¸²à¸ˆà¸°à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
    delete_data_option = st.checkbox("à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥", value=False)

    if delete_data_option:
        # à¹à¸ªà¸”à¸‡à¸Šà¹ˆà¸­à¸‡à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸¡à¸·à¹ˆà¸­à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸•à¸´à¹Šà¸à¹€à¸¥à¸·à¸­à¸
        st.header("à¹€à¸¥à¸·à¸­à¸à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥")
        delete_start_date = st.date_input("à¸à¸³à¸«à¸™à¸”à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥", value=start_date, key='delete_start')
        delete_start_time = st.time_input("à¹€à¸§à¸¥à¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™", value=pd.Timestamp("00:00:00").time(), key='delete_start_time')
        delete_end_date = st.date_input("à¸à¸³à¸«à¸™à¸”à¸ªà¸´à¹‰à¸™à¸ªà¸¸à¸”à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥", value=end_date, key='delete_end')
        delete_end_time = st.time_input("à¹€à¸§à¸¥à¸²à¸ªà¸´à¹‰à¸™à¸ªà¸¸à¸”", value=pd.Timestamp("23:45:00").time(), key='delete_end_time')

    process_button = st.button("à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥")

# Main content: Display results after file uploads and date selection
if uploaded_file and uploaded_file2:
    df = load_data(uploaded_file)
    df_pre = clean_data(df)

    df2 = load_data(uploaded_file2)
    df2_pre = clean_data(df2)

    plot_data_preview(df_pre, df2_pre)

    if process_button:
        processing_placeholder = st.empty()
        processing_placeholder.text("à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸‚à¹‰à¸­à¸¡à¸¹à¸¥...")

        df['datetime'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)

        # à¸›à¸£à¸±à¸šà¸„à¹ˆà¸² end_date à¹€à¸‰à¸à¸²à¸°à¸–à¹‰à¸²à¹€à¸¥à¸·à¸­à¸à¸Šà¹ˆà¸§à¸‡à¹€à¸§à¸¥à¸²à¹à¸¥à¹‰à¸§
        end_date = end_date + pd.DateOffset(days=1)

        # à¸à¸£à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡à¸Šà¹ˆà¸§à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆà¹€à¸¥à¸·à¸­à¸
        df_filtered = df[(df['datetime'] >= pd.to_datetime(start_date)) & (df['datetime'] <= pd.to_datetime(end_date))]

        # Clean data
        df_clean = clean_data(df_filtered)

        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¹€à¸¥à¸·à¸­à¸à¸—à¸µà¹ˆà¸ˆà¸°à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
        if delete_data_option:
            delete_start_datetime = pd.to_datetime(f"{delete_start_date} {delete_start_time}")
            delete_end_datetime = pd.to_datetime(f"{delete_end_date} {delete_end_time}")
            df_deleted = delete_data_by_date_range(df_clean, delete_start_datetime, delete_end_datetime)
        else:
            df_deleted = df_clean.copy()  # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹€à¸¥à¸·à¸­à¸à¸¥à¸šà¸à¹‡à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡à¹à¸—à¸™

        # Generate all dates
        df_clean = generate_missing_dates(df_clean)

        # Fill NaN values in 'code' column
        df_clean = fill_code_column(df_clean)

        # Create time features
        df_clean = create_time_features(df_clean)

        # à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¸à¸²à¸£à¸ªà¸¸à¹ˆà¸¡à¸¥à¸š
        df_before_random_deletion = df_filtered.copy()

        # Handle missing values by week
        df_handled = handle_missing_values_by_week(df_clean, start_date, end_date)

        # Remove the processing message after the processing is complete
        processing_placeholder.empty()

        # Plot the results using Streamlit's line chart
        plot_results(df_before_random_deletion, df_handled, df_deleted)
    st.markdown("---")